{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "mobile-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.sparse as sparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import importlib\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from network import vlayers\n",
    "importlib.reload(vlayers)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "abandoned-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(inputs, dims_to_flatten, dims_to_save=0):\n",
    "    \"\"\"Flatten given dimensions of tensor\"\"\"\n",
    "    input_shape = inputs.shape\n",
    "    rank = input_shape.rank\n",
    "    batch_dims = input_shape[:rank-dims_to_flatten]\n",
    "    non_batch_dims = input_shape[-dims_to_flatten:]\n",
    "    \n",
    "    if dims_to_save > 0:\n",
    "        saved_dims = input_shape[-1:]\n",
    "    \n",
    "    if tf.executing_eagerly():\n",
    "        # Full static shape is guaranteed to be available.\n",
    "        # Performance: Using `constant_op` is much faster than passing a list.\n",
    "        if dims_to_save:\n",
    "            flattened_shape = tf.concat([batch_dims, [-1], saved_dims], 0)\n",
    "        else:\n",
    "            flattened_shape = tf.concat([batch_dims, [-1]], 0)\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "    else:\n",
    "        last_dim = int(functools.reduce(operator.mul, non_batch_dims))\n",
    "        if dims_to_save:\n",
    "            flattened_shape = tf.concat([[-1], batch_dims[1:], [last_dim], saved_dims], 0)\n",
    "        else:\n",
    "            flattened_shape = tf.concat([[-1], batch_dims[1:], [last_dim]], 0)\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "\n",
    "def concat_biases(inputs, axis=-1):\n",
    "    \"\"\"Add bias to each input vector\"\"\"\n",
    "    inputs_rank = len(inputs.shape)\n",
    "    # Inputs shape can be partially known, so\n",
    "    # Get inputs slice with current dimension equals one\n",
    "    slice_begin = tf.zeros(inputs_rank, dtype=tf.int32)\n",
    "    slice_size = tf.concat([tf.fill([inputs_rank + axis], -1), tf.constant([1]), tf.fill([-axis - 1], -1)], 0)\n",
    "    inputs_slice = tf.slice(inputs, slice_begin, slice_size)\n",
    "    # Create biases shaped like inputs slice\n",
    "    biases = tf.ones_like(inputs_slice, dtype=inputs.dtype)\n",
    "    # Concatenate inputs with biases\n",
    "    x = tf.concat([inputs, biases], axis)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "def get_input_shape(input_shape, paddings):\n",
    "    \"\"\"Get shape of input feature tensor\"\"\"\n",
    "    input_shape = tf.constant(input_shape[-3:])\n",
    "    if paddings is not None:\n",
    "        paddings = tf.reduce_sum(paddings[-3:], axis=-1)\n",
    "        input_shape += paddings\n",
    "    return input_shape\n",
    "\n",
    "def get_full_output_shape(input_shape, kernel_shape, strides, paddings, use_bias):\n",
    "    \"\"\"Get shape of output tensor\"\"\"\n",
    "    input_shape = get_input_shape(input_shape, paddings)\n",
    "    vector_dim = tf.reduce_prod(kernel_shape[:-1])\n",
    "    if use_bias:\n",
    "        vector_dim += 1\n",
    "    \n",
    "    filters_num = kernel_shape[-1]\n",
    "    \n",
    "    input_shape = tf.constant(input_shape[-3:])\n",
    "    kernel_shape = tf.constant(kernel_shape[-4:-1])\n",
    "    strides = tf.concat([tf.constant(strides), [1]], axis=0)\n",
    "    \n",
    "    # Convolution layer output shape formula\n",
    "    output_shape = ((input_shape - kernel_shape) // strides) + 1\n",
    "    # Add filters \n",
    "    output_shape *= tf.concat([1, 1, filters_num], axis=0)\n",
    "    # Add vector dimension\n",
    "    output_shape = tf.concat([[-1, vector_dim], output_shape], axis=0)\n",
    "    return output_shape\n",
    "\n",
    "def get_output_shape_for_single_filter(input_shape, kernel_shape, strides):\n",
    "    \"\"\"Get shape of output feature tensor for single filter\"\"\"\n",
    "    input_shape = tf.constant(input_shape[-3:])\n",
    "    kernel_shape = tf.constant(kernel_shape[-4:-1])\n",
    "    strides = tf.concat([tf.constant(strides), [1]], axis=0)\n",
    "    # Convolution layer output shape formula\n",
    "    output_shape = ((input_shape - kernel_shape) // strides) + 1\n",
    "    return output_shape\n",
    "\n",
    "def iterate_input_gather_indices(weight_shape, input_shape, output_shape, kernel_shape, strides, use_bias, is_vector_input):\n",
    "    \"\"\"Iterate over indices of elements (to extract from input/activization tensor) for furhter multiplication by weights\"\"\"\n",
    "    # Amounts of already taken elements in flattened input\n",
    "    position_nums = dict()\n",
    "    \n",
    "    for j in range(weight_shape[-2]):\n",
    "        # Compute position in output tensor\n",
    "        chan_num = j % output_shape[-1]\n",
    "        col_num = (j // output_shape[-1]) % output_shape[-2]\n",
    "        row_num = (j // output_shape[-1]) // output_shape[-2]\n",
    "        # Compute row in weight tensor to start with\n",
    "        offset = (row_num * input_shape[-2] * strides[-2] + col_num \\\n",
    "                * strides[-1]) * input_shape[-1] + chan_num\n",
    "        \n",
    "        for i in range(tf.cast(weight_shape[-3], tf.int32) - offset):\n",
    "            # Compute position in input tensor\n",
    "            chan_num = i % input_shape[-1]\n",
    "            col_num = (i // input_shape[-1]) % input_shape[-2]\n",
    "            row_num = (i // input_shape[-1]) // input_shape[-2]\n",
    "            \n",
    "            if (chan_num < kernel_shape[-2] and col_num < kernel_shape[-3] and row_num < kernel_shape[-4]):\n",
    "                position = (i + int(offset),)\n",
    "                if is_vector_input:\n",
    "                    if position not in position_nums:\n",
    "                        position_nums[position] = 0\n",
    "                    yield position + (position_nums[position],)\n",
    "                    position_nums[position] += 1\n",
    "                else:\n",
    "                    yield position\n",
    "        # Set bias\n",
    "        if use_bias:\n",
    "            if is_vector_input:\n",
    "                yield (int(weight_shape[-3]-1), 0)\n",
    "            else:\n",
    "                yield (int(weight_shape[-3]-1),)\n",
    "\n",
    "def get_input_gather_indices(input_shape, kernel_shape, strides, paddings, use_bias, is_vector_input):\n",
    "    \"\"\"Iterate over indices of elements (to extract from input/activization tensor) for furhter multiplication by weights\"\"\"\n",
    "    # Padded input shape\n",
    "    input_shape = get_input_shape(input_shape, paddings)\n",
    "    # Output shape\n",
    "    output_shape = get_output_shape_for_single_filter(input_shape, kernel_shape, strides)\n",
    "    output_flat_len = np.prod(output_shape)\n",
    "    \n",
    "    # Compute all offsets\n",
    "    row_offsets = np.arange(output_shape[-3]) * input_shape[-2].numpy() * strides[-2] * input_shape[-1].numpy()\n",
    "    col_offsets = np.arange(output_shape[-2]) * strides[-1] * input_shape[-1].numpy()\n",
    "    chan_offsets = np.arange(output_shape[-1])\n",
    "    offsets = row_offsets[:, np.newaxis, np.newaxis] + col_offsets[:, np.newaxis] + chan_offsets\n",
    "    offsets = offsets.flatten()\n",
    "    \n",
    "    # Find gathering indices without offset\n",
    "    index_bool_vector = np.zeros(input_shape)\n",
    "    index_bool_vector[:kernel_shape[-4],:kernel_shape[-3],:kernel_shape[-2]] = 1\n",
    "    index_bool_vector = index_bool_vector.flatten()\n",
    "    index_vector = np.nonzero(index_bool_vector)[0]\n",
    "    index_matrix = index_vector[:, np.newaxis]\n",
    "    index_matrix = np.tile(index_matrix, (1, output_flat_len))\n",
    "    \n",
    "    # Find gathering indices with offsets\n",
    "    offset_index_matrix = index_matrix + offsets\n",
    "    \n",
    "    # Append bias\n",
    "    if use_bias:\n",
    "        bias_index = np.prod(input_shape)\n",
    "        bias = np.full((1, offset_index_matrix.shape[-1]), bias_index, dtype=np.int32)\n",
    "        offset_index_matrix = np.concatenate((offset_index_matrix, bias), axis=0)\n",
    "    \n",
    "    # Set vector indices for all inputs\n",
    "    if is_vector_input:\n",
    "        row_num = index_bool_vector.shape[0]\n",
    "        col_num = output_flat_len\n",
    "        row_indices = np.arange(row_num)[:,np.newaxis]\n",
    "        col_indices = np.arange(col_num)\n",
    "        shifts = np.mod(row_indices - offsets, row_num)\n",
    "        unshifts = np.mod(row_indices + offsets, row_num)\n",
    "        # Set indices\n",
    "        uniqueness_indices = index_bool_vector[:, np.newaxis]\n",
    "        uniqueness_indices = np.tile(uniqueness_indices, (1, col_num))\n",
    "        # Shift by offsets\n",
    "        uniqueness_indices = uniqueness_indices[shifts, col_indices]\n",
    "        # Get unique indices\n",
    "        uniqueness_indices = np.cumsum(uniqueness_indices, axis=1)\n",
    "        # Unshift\n",
    "        uniqueness_indices = uniqueness_indices[unshifts, col_indices]\n",
    "        # Slice and convert to zero-based indexing\n",
    "        uniqueness_indices = uniqueness_indices[index_vector] - 1\n",
    "        # Append bias\n",
    "        if use_bias:\n",
    "            bias = np.full((1, uniqueness_indices.shape[-1]), 0, dtype=np.int32)\n",
    "            uniqueness_indices = np.concatenate((uniqueness_indices, bias), axis=0)\n",
    "        \n",
    "        # Set unique vector indices\n",
    "        offset_index_matrix = np.stack((offset_index_matrix, uniqueness_indices), axis=-1)\n",
    "    else:\n",
    "        offset_index_matrix = offset_index_matrix[...,np.newaxis]\n",
    "    \n",
    "    return tf.constant(offset_index_matrix, dtype=tf.int64)\n",
    "        \n",
    "                \n",
    "def get_input_gather_indices_slow(input_shape, kernel_shape, strides, paddings, use_bias, is_vector_input):\n",
    "    # Padded input shape\n",
    "    input_shape = get_input_shape(input_shape, paddings)\n",
    "    # Output shape\n",
    "    output_shape = get_output_shape_for_single_filter(input_shape, kernel_shape, strides)\n",
    "    \n",
    "    # Shape of sparsed weight tensor (which actually is not used)\n",
    "    input_flat_len = tf.reduce_prod(input_shape) + (1 if use_bias else 0)\n",
    "    output_flat_len = tf.reduce_prod(output_shape)\n",
    "    filters_len = tf.reduce_prod(kernel_shape[-1:])\n",
    "    weight_shape = tf.concat([input_flat_len, output_flat_len, filters_len], axis=0)\n",
    "    \n",
    "    # Get indices of elements to take from input to avoid slow multiplication to sparsed tensor\n",
    "    gather_indices = tf.constant(\n",
    "        list(iterate_input_gather_indices(\n",
    "            weight_shape, \n",
    "            input_shape, \n",
    "            output_shape, \n",
    "            kernel_shape, \n",
    "            strides, \n",
    "            use_bias, \n",
    "            is_vector_input)\n",
    "        ),\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    # Reshape indices to appropriate tensor\n",
    "    vector_dim = tf.reduce_prod(kernel_shape[:-1])\n",
    "    if use_bias:\n",
    "        # Bias must be taken into account\n",
    "        vector_dim += 1\n",
    "    gather_shape = tf.concat([tf.reduce_prod(output_shape), vector_dim, tf.constant(2 if is_vector_input else 1)], axis=0)\n",
    "    gather_indices = tf.reshape(gather_indices, gather_shape)\n",
    "    gather_indices = tf.transpose(gather_indices, perm=[1,0,2])\n",
    "    \n",
    "    return gather_indices\n",
    "\n",
    "def apply_convolution_kernel(x, flattened_weight, weight_tiles, gather_indices, paddings, use_bias, output_shape, is_vector_input):\n",
    "    \"\"\"Apply convolution kernel to input/activation\"\"\"\n",
    "    # Pad\n",
    "    if paddings is not None:\n",
    "        x = tf.pad(x, paddings)\n",
    "    # Flatten\n",
    "    x_flat = flatten(x, tf.constant(3), 1 if is_vector_input else 0)\n",
    "    if use_bias:\n",
    "        x_flat = concat_biases(x_flat, axis=-1)\n",
    "    # Rearrange\n",
    "    x = tf.transpose(x_flat, perm=[1,0])\n",
    "    x = tf.gather_nd(x, gather_indices)\n",
    "    x = tf.transpose(x, perm=[2,0,1])\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    # Fill weight tensor\n",
    "    weight = tf.tile(flattened_weight, weight_tiles)\n",
    "    # Multiply \n",
    "    y = x * weight\n",
    "    # Reshape\n",
    "    return tf.reshape(y, output_shape)\n",
    "\n",
    "def apply_padding_kernel(x, gather_indices, paddings, use_bias, output_shape):\n",
    "    \"\"\"Apply padding (pseudo)kernel to input/activation\"\"\"\n",
    "    # Pad\n",
    "    if paddings is not None:\n",
    "        x = tf.pad(x, paddings)\n",
    "    # Flatten\n",
    "    x_flat = flatten(x, tf.constant(3))\n",
    "    if use_bias:\n",
    "        x_flat = concat_biases(x_flat, axis=-1)\n",
    "    # Rearrange\n",
    "    x = tf.transpose(x_flat, perm=[1,0])\n",
    "    x = tf.gather_nd(x, gather_indices)\n",
    "    x = tf.transpose(x, perm=[2,0,1])\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    # Reshape\n",
    "    return tf.reshape(x, output_shape)\n",
    "\n",
    "def do_convolution(x, activation_fun):\n",
    "    \"\"\"Do simple convolution over input\"\"\"\n",
    "    # Sum\n",
    "    s = tf.reduce_sum(x, -4)\n",
    "    # Compute activation\n",
    "    a = activation_fun(s)\n",
    "    \n",
    "    return a\n",
    "\n",
    "def do_convolution_with_inner_net(x, inner_input, inner_hiddens, inner_output):\n",
    "    \"\"\"Do convolution over input, using inner fractal network\"\"\"\n",
    "    # Set vector input as last dimension\n",
    "    x = tf.transpose(x, perm=[0,2,3,4,1])\n",
    "    # Process data as inner network\n",
    "    y = self.inner_input(x)\n",
    "    for hidden in self.inner_hiddens:\n",
    "        y = hidden(y)\n",
    "    a = self.inner_output(y)\n",
    "    \n",
    "    return a\n",
    "\n",
    "def do_pooling(x, pooling_fun):\n",
    "    \"\"\"Do pooling over input\"\"\"\n",
    "    s = pooling_fun(x, -4)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "happy-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VInputConv(layers.Layer):\n",
    "    \"\"\"Input vector layer for convolutional networks\"\"\"\n",
    "    def __init__(self, filter_shape, num_filters=1, kernel_type=\"convolution\", strides=(1,1), padding_type=\"valid\", weight_initializer=\"random_normal\"):\n",
    "        super().__init__()\n",
    "        self.filter_shape = filter_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_type = kernel_type\n",
    "        self.strides = strides\n",
    "        self.padding_type = padding_type\n",
    "        self.weight_initializer = weight_initializer\n",
    "        self.is_vector_input = False\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_rank = input_shape.rank\n",
    "        \n",
    "        # Set kernel shape\n",
    "        if self.kernel_type == \"convolution\":\n",
    "            kernel_shape = tf.concat([self.filter_shape, input_shape[-1:], [self.num_filters]], axis=0)\n",
    "            use_bias = True\n",
    "            bias_shape = kernel_shape[-1:]\n",
    "        else: # elif kernel_type == \"pooling\"\n",
    "            kernel_shape = tf.concat([self.filter_shape, [1, 1]], axis=0)\n",
    "            use_bias = False\n",
    "        \n",
    "        # Init padding\n",
    "        if self.padding_type == \"same\":\n",
    "            paddings = tf.constant(kernel_shape[-4:-2]) - 1\n",
    "            paddings_before = paddings // 2\n",
    "            paddings_after = paddings - paddings_before\n",
    "            paddings_before = tf.concat([tf.zeros([input_rank - 3], dtype=tf.int32), paddings_before, [0]], axis=0)\n",
    "            paddings_after = tf.concat([tf.zeros([input_rank - 3], dtype=tf.int32), paddings_after, [0]], axis=0)\n",
    "            paddings = tf.stack([paddings_before, paddings_after], axis=1)\n",
    "        elif self.padding_type == \"full\":\n",
    "            paddings = tf.constant(kernel_shape[-4:-2]) - 1\n",
    "            paddings = tf.concat([tf.zeros([input_rank - 3], dtype=tf.int32), paddings, [0]], axis=0)\n",
    "            paddings = tf.stack([paddings, paddings], axis=1)\n",
    "        else: # elif self.padding_type == \"valid\"\n",
    "            paddings = None\n",
    "        \n",
    "        # Set output shape\n",
    "        self.output_shape = get_full_output_shape(input_shape, kernel_shape, self.strides, paddings, use_bias)\n",
    "        \n",
    "        # Get indices to gather elements input/activation tensor into appropriate shape (rearrange)\n",
    "#         gather_indices = get_input_gather_indices_slow(\n",
    "#             input_shape, \n",
    "#             kernel_shape, \n",
    "#             self.strides, \n",
    "#             paddings, \n",
    "#             use_bias, \n",
    "#             self.is_vector_input\n",
    "#         )\n",
    "        gather_indices = get_input_gather_indices(\n",
    "            input_shape, \n",
    "            kernel_shape, \n",
    "            self.strides, \n",
    "            paddings, \n",
    "            use_bias, \n",
    "            self.is_vector_input\n",
    "        )\n",
    "        \n",
    "        if self.kernel_type == \"convolution\":\n",
    "            # Init weights and biases (already flattened)\n",
    "            flattened_weight_shape = tf.concat(\n",
    "                [\n",
    "                    tf.reduce_prod(kernel_shape[:-1]) + 1 if use_bias else 0, \n",
    "                    1, \n",
    "                    kernel_shape[-1]\n",
    "                ], \n",
    "                axis=0\n",
    "            )\n",
    "            self.flattened_weight = self.add_weight(\n",
    "                shape=flattened_weight_shape,\n",
    "                initializer=self.weight_initializer\n",
    "            )\n",
    "#             if self.kernel is None:\n",
    "#                 self.kernel = self.add_weight(\n",
    "#                     shape=kernel_shape,\n",
    "#                     initializer=self.weight_initializer\n",
    "#                 )\n",
    "#                 if use_bias:\n",
    "#                     self.bias = self.add_weight(\n",
    "#                         shape=bias_shape,\n",
    "#                         initializer=self.weight_initializer\n",
    "#                     )\n",
    "\n",
    "#             # Get flattened kernel\n",
    "#             self.flattened_weight = tf.reshape(self.kernel, tf.concat([tf.reduce_prod(self.kernel.shape[:-1]), 1, self.kernel.shape[-1]], axis=0))\n",
    "#             if use_bias:\n",
    "#                 # Get bias values\n",
    "#                 bias = tf.reshape(self.bias, tf.concat([1, 1, self.bias.shape[-1]], axis=0))\n",
    "#                 self.flattened_weight = tf.concat([self.flattened_weight, bias], axis=0)\n",
    "            \n",
    "            # Dimensions to tile weights\n",
    "            weight_tiles = tf.concat([[1], gather_indices.shape[-2:-1], [1]], axis=0)\n",
    "            \n",
    "            self.weight_multiply_fun = lambda x: apply_convolution_kernel(\n",
    "                x, self.flattened_weight, weight_tiles, gather_indices, paddings, use_bias, self.output_shape\n",
    "            )\n",
    "        else: # elif kernel_type == \"pooling\"\n",
    "            self.weight_multiply_fun = lambda x: apply_padding_kernel(\n",
    "                x, gather_indices, paddings, use_bias, self.output_shape\n",
    "            )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.weight_multiply_fun(inputs)\n",
    "\n",
    "        \n",
    "class VConv(VInputConv):\n",
    "    \"\"\"Hidden vector convolution layer\"\"\"\n",
    "    def __init__(self, filter_shape, num_filters=1, kernel_type=\"convolution\", strides=(1,1), padding_type=\"valid\", weight_initializer=\"random_normal\", layer_type=\"convolution\", activation=\"relu\", pooling=\"max\"):\n",
    "        super().__init__(filter_shape, num_filters, kernel_type, strides, padding_type, weight_initializer)\n",
    "        \n",
    "        self.layer_type = layer_type\n",
    "        self.activation = activation\n",
    "        self.pooling = pooling\n",
    "        self.activation_fun = tf.keras.activations.deserialize(activation)\n",
    "        if pooling == \"max\":\n",
    "            self.pooling_fun = tf.math.reduce_max\n",
    "        elif pooling == \"mean\":\n",
    "            self.pooling_fun = tf.math.reduce_mean\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        \n",
    "        if self.layer_type == \"convolution\":\n",
    "            self.op_fun = lambda x: do_convolution(x, self.activation_fun, self.is_vector_input)\n",
    "        else: # elif self.layer_type == \"pooling\"\n",
    "            self.op_fun = lambda x: do_pooling(x, self.pooling_fun)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.op_fun(inputs)\n",
    "        return self.weight_multiply_fun(x)\n",
    "    \n",
    "\n",
    "class VConvFractal(VConv):\n",
    "    \"\"\"Hidden vector convolution layer with inner networks\"\"\"\n",
    "    def __init__(self, filter_shape, num_filters=1, kernel_type=\"convolution\", strides=(1,1), padding_type=\"valid\", weight_initializer=\"random_normal\", layer_type=\"convolution\", activation=\"relu\", pooling=\"max\", depth=1, shared_inner_nets=False, hidden_layer_units=(2,)):\n",
    "        super().__init__(\n",
    "            filter_shape, \n",
    "            num_filters, \n",
    "            kernel_type, \n",
    "            strides, \n",
    "            padding_type, \n",
    "            weight_initializer, \n",
    "            layer_type, \n",
    "            activation, \n",
    "            pooling\n",
    "        )\n",
    "        self.is_vector_input = True\n",
    "        self.depth = depth\n",
    "        self.shared_inner_nets = shared_inner_nets\n",
    "        self.hidden_layer_units = hidden_layer_units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        \n",
    "        # Initialize inner network\n",
    "        if layer_type == \"convolution\" and self.depth > 0:\n",
    "            input_item_rank = 2 if self.shared_inner_nets else 1\n",
    "            output_units = self.output_shape[-4]\n",
    "            \n",
    "            self.inner_input = VInput(self.hidden_layer_units[0], \n",
    "                                      input_item_rank=input_item_rank,\n",
    "                                      add_bias=False, \n",
    "                                      weight_initializer=self.weight_initializer,\n",
    "                                      weight_type=\"unique\")\n",
    "            self.inner_hiddens = [VFractal(u, \n",
    "                                           input_item_rank = input_item_rank,\n",
    "                                           depth=self.depth - 1,\n",
    "                                           hidden_layer_units=self.hidden_layer_units,\n",
    "                                           activation=self.activation,\n",
    "                                           weight_initializer=self.weight_initializer,\n",
    "                                           weight_type=\"unique\") \n",
    "                                  for u \n",
    "                                  in self.hidden_layer_units[1:] + (output_units,)]\n",
    "            self.inner_output = VOutput(activation=self.activation)\n",
    "            \n",
    "            self.op_fun = lambda x: do_convolution_with_inner_net(x, self.inner_input, self.inner_hiddens, self.inner_output)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs)\n",
    "        \n",
    "    \n",
    "class VOutputConv(layers.Layer):\n",
    "    \"\"\"Output vector layer for convolutional networks\"\"\"\n",
    "    def __init__(self, layer_type=\"convolution\", activation=\"relu\", pooling=\"max\"):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.activation = activation\n",
    "        self.pooling = pooling\n",
    "        self.activation_fun = tf.keras.activations.deserialize(activation)\n",
    "        if pooling == \"max\":\n",
    "            self.pooling_fun = tf.math.reduce_max\n",
    "        elif pooling == \"mean\":\n",
    "            self.pooling_fun = tf.math.reduce_mean\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.layer_type == \"convolution\":\n",
    "            self.op_fun = lambda x: do_convolution(x, self.activation_fun)\n",
    "        else: # elif self.layer_type == \"pooling\"\n",
    "            self.op_fun = lambda x: do_pooling(x, self.pooling_fun)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.op_fun(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "affecting-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "batch_size = 64\n",
    "x_dim = 32\n",
    "x_shape=3\n",
    "x = tf.reshape(tf.range([batch_size*x_dim*x_dim*x_shape], dtype=tf.float32), shape=(batch_size,x_dim,x_dim,x_shape))\n",
    "strides = (1,1)\n",
    "padding = \"same\"\n",
    "num_filters=12\n",
    "filter_dim=7\n",
    "\n",
    "tries = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "featured-right",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 31, 31, 12)\n"
     ]
    }
   ],
   "source": [
    "platform_layer1 = layers.Conv2D(num_filters, filter_dim, activation='relu', strides=strides, padding=padding)\n",
    "platform_layer2 = tf.keras.layers.ReLU()\n",
    "platform_layer3 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(1, 1))\n",
    "x1 = platform_layer1(x)\n",
    "x1 = platform_layer2(x1)\n",
    "x1 = platform_layer3(x1)\n",
    "print(x1.shape)\n",
    "\n",
    "def platform_net():\n",
    "    x1 = platform_layer1(x)\n",
    "    x1 = platform_layer2(x1)\n",
    "    x1 = platform_layer3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "another-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(platform_layer1.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "thorough-child",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 2), dtype=float32, numpy=\n",
       "array([[[[3.218261 , 4.119361 ],\n",
       "         [0.       , 4.119361 ]],\n",
       "\n",
       "        [[5.401186 , 3.3237386],\n",
       "         [0.       , 1.6719575]]]], dtype=float32)>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlayer1 = VInputConv((filter_dim, filter_dim), num_filters, \"convolution\", strides, padding_type=padding)\n",
    "# vlayer1.kernel = tf.reshape(tf.range(filter_dim*filter_dim*x_shape*num_filters, dtype=tf.float32)*0.01, shape=(filter_dim,filter_dim,x_shape,num_filters))\n",
    "# print(platform_layer1.get_weights()[0].shape)\n",
    "# print(tf.reshape(tf.range(filter_dim*filter_dim*x_shape*num_filters, dtype=tf.float32)*0.01, shape=(filter_dim,filter_dim,x_shape,num_filters)).shape)\n",
    "vlayer1.kernel = platform_layer1.get_weights()[0]\n",
    "# vlayer1.bias = tf.ones([num_filters], dtype=tf.float32)\n",
    "vlayer1.bias = platform_layer1.get_weights()[1]\n",
    "vlayer2 = VConv((2,2), kernel_type=\"pooling\", strides=(1,1), padding_type=\"valid\", layer_type=\"convolution\", activation=\"relu\")\n",
    "vlayer3  = VOutputConv(layer_type=\"pooling\", pooling=\"max\")\n",
    "x1 = vlayer1(x)\n",
    "x1 = vlayer2(x1)\n",
    "vlayer3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "graphic-productivity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 31, 31, 12)\n"
     ]
    }
   ],
   "source": [
    "vlayer1 = VInputConv((filter_dim, filter_dim), num_filters, \"convolution\", strides, padding_type=padding)\n",
    "vlayer2 = VConv((2,2), kernel_type=\"pooling\", strides=(1,1), padding_type=\"valid\", layer_type=\"convolution\", activation=\"relu\")\n",
    "vlayer3  = VOutputConv(layer_type=\"pooling\", pooling=\"max\")\n",
    "# v_time = timeit.timeit(lambda: print(vlayer3(vlayer2(vlayer1(x))).shape), number=1)\n",
    "x1 = vlayer1(x)\n",
    "x1 = vlayer2(x1)\n",
    "x1 = vlayer3(x1)\n",
    "print(x1.shape)\n",
    "\n",
    "def vnet():\n",
    "    x1 = vlayer1(x)\n",
    "    x1 = vlayer2(x1)\n",
    "    x1 = vlayer3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "radical-remove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794.1633896999992"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "hairy-coordinate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.315365029161324\n"
     ]
    }
   ],
   "source": [
    "v_time = timeit.timeit(vnet, number=tries)\n",
    "platform_time  = timeit.timeit(platform_net, number=tries)\n",
    "\n",
    "print(v_time / platform_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
