{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "import tensorflow.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "hollywood-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(inputs, dims_to_flatten):\n",
    "    input_shape = inputs.shape\n",
    "    rank = input_shape.rank\n",
    "    batch_dims = input_shape[:rank-dims_to_flatten]\n",
    "    non_batch_dims = input_shape[-dims_to_flatten:]\n",
    "    \n",
    "    if tf.executing_eagerly():\n",
    "        # Full static shape is guaranteed to be available.\n",
    "        # Performance: Using `constant_op` is much faster than passing a list.\n",
    "        flattened_shape = tf.concat([batch_dims, [-1]], 0)\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "    else:\n",
    "        last_dim = int(functools.reduce(operator.mul, non_batch_dims))\n",
    "        flattened_shape = tf.concat([[-1], batch_dims[1:], [last_dim]])\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "\n",
    "def get_conv_fun(input_shape, kernel_shape, strides):\n",
    "    # Get spatial shape\n",
    "    def get_result_spat_shape(input_spat_shape, kernel_spat_shape, strides):\n",
    "        \n",
    "        return ((input_spat_shape - kernel_spat_shape) // strides) + 1\n",
    "    # Get sparced indices generator\n",
    "    def iterate_sparsed_indices(sparsed_shape, input_shape, kernel_shape, result_shape, strides):\n",
    "        for i in range(sparsed_shape[-2]):\n",
    "            for j in range(sparsed_shape[-1]):\n",
    "                channel_num = i % result_shape[-1]\n",
    "                col_num = (i // result_shape[-1]) % result_shape[-2]\n",
    "                row_num = (i // result_shape[-1]) // result_shape[-2]\n",
    "                offset = (row_num * input_shape[-2] + col_num) * input_shape[-1] * strides + channel_num\n",
    "                if j >= offset and \\\n",
    "                    ((j - offset) % input_shape[-1]) < kernel_shape[-1] and \\\n",
    "                    (((j - offset) // input_shape[-1]) % input_shape[-2]) < kernel_shape[-2] and \\\n",
    "                    (((j - offset) // input_shape[-1]) // input_shape[-2]) < kernel_shape[-3]:\n",
    "                    yield [i, j]\n",
    "    \n",
    "    # Get height and width of result tensor\n",
    "    result_spat_shape = ((tf.constant(input_shape[-3:-1]) - tf.constant(kernel_shape[-3:-1])) // strides) + 1\n",
    "    # Get depth of result tensor (for pooling filter, strides=1, kernel depth (supposely) =1)\n",
    "    result_depth_shape = (tf.constant(input_shape[-1:]) - tf.constant(kernel_shape[-1:])) + 1\n",
    "    result_shape = tf.concat([\n",
    "        input_shape[:-3],\n",
    "        result_spat_shape,\n",
    "        result_depth_shape\n",
    "    ], 0)\n",
    "    \n",
    "    input_flat_len = tf.reduce_prod(tf.constant(input_shape[-3:]))\n",
    "    result_flat_len = tf.reduce_prod(tf.constant(result_shape[-3:]))\n",
    "    \n",
    "    sparsed_shape = tf.concat([result_flat_len, input_flat_len], axis=0)\n",
    "    sparsed_shape = tf.cast(sparsed_shape, tf.int64)\n",
    "    \n",
    "    sparsed_indices = tf.constant(\n",
    "        list(iterate_sparsed_indices(sparsed_shape, input_shape, kernel_shape, result_shape, strides)),\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    def conv_fun(inputs, kernel, bias):\n",
    "        nonlocal sparsed_shape\n",
    "        nonlocal sparsed_indices\n",
    "        \n",
    "        sparsed_values = tf.reshape(kernel, [-1])\n",
    "        sparsed_values = tf.tile(sparsed_values, sparsed_shape[:1])\n",
    "        \n",
    "        sparsed_kernel = sparse.SparseTensor(sparsed_indices, sparsed_values, sparsed_shape)\n",
    "        \n",
    "        input_flat = tf.expand_dims(flatten(inputs, tf.constant(3)), -1)\n",
    "        \n",
    "        return tf.sparse.sparse_dense_matmul(sparsed_kernel, input_flat) + bias\n",
    "    \n",
    "    return conv_fun\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "beginning-merit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n",
       "array([[ 55.],\n",
       "       [ 65.],\n",
       "       [ 75.],\n",
       "       [ 85.],\n",
       "       [115.],\n",
       "       [125.],\n",
       "       [135.],\n",
       "       [145.]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reshape(tf.range([3*3*2], dtype=tf.float32), shape=(3,3,2))\n",
    "# kernel = tf.constant([[[1,2], [3,4]], [[5,6], [7,8]]], dtype=tf.float32)\n",
    "# kernel = tf.constant([[[1,2],[3,4],[5,6]], [[7,8],[9,10],[11,12]]], dtype=tf.float32)\n",
    "kernel = tf.constant([[[1], [2]], [[3], [4]]], dtype=tf.float32)\n",
    "strides = 1\n",
    "bias = 1\n",
    "conv_fun = get_conv_fun(x.shape, kernel.shape, strides)\n",
    "conv_fun(x, kernel, bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
