{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "greater-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.sparse as sparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import importlib\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from network import vlayers\n",
    "importlib.reload(vlayers)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "postal-force",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]\n",
      "\n",
      " [[12 13]\n",
      "  [14 15]\n",
      "  [16 17]]\n",
      "\n",
      " [[18 19]\n",
      "  [20 21]\n",
      "  [22 23]]], shape=(4, 3, 2), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
       "array([[[0, 1],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[6, 7],\n",
       "        [8, 9]]])>"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.reshape(tf.range(24), [4, 3, 2])\n",
    "print(data)\n",
    "x = tf.constant(data)\n",
    "result = tf.gather_nd(x, [[[0, 0], [0, 1]], [[1, 0], [1, 1]]], batch_dims=0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "opposite-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(inputs, dims_to_flatten):\n",
    "    \"\"\"Flatten given dimensions of tensor\"\"\"\n",
    "    input_shape = inputs.shape\n",
    "    rank = input_shape.rank\n",
    "    batch_dims = input_shape[:rank-dims_to_flatten]\n",
    "    non_batch_dims = input_shape[-dims_to_flatten:]\n",
    "    \n",
    "    if tf.executing_eagerly():\n",
    "        # Full static shape is guaranteed to be available.\n",
    "        # Performance: Using `constant_op` is much faster than passing a list.\n",
    "        flattened_shape = tf.concat([batch_dims, [-1]], 0)\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "    else:\n",
    "        last_dim = int(functools.reduce(operator.mul, non_batch_dims))\n",
    "        flattened_shape = tf.concat([[-1], batch_dims[1:], [last_dim]])\n",
    "        return tf.reshape(inputs, flattened_shape)\n",
    "    \n",
    "def get_input_shape(input_shape, padding):\n",
    "    \"\"\"Get shape of input feature tensor\"\"\"\n",
    "    input_shape = tf.constant(input_shape)\n",
    "    return tf.concat([input_shape[-3:-1] + 2 * padding, input_shape[-1:]], axis=0)\n",
    "\n",
    "def get_full_output_shape(input_shape, kernel_shape, strides, use_bias):\n",
    "    \"\"\"Get shape of output tensor\"\"\"\n",
    "    vector_dim = tf.reduce_prod(kernel_shape[:-1])\n",
    "    if use_bias:\n",
    "        vector_dim += 1\n",
    "    input_shape = tf.constant(input_shape[-3:])\n",
    "    kernel_shape = tf.constant(kernel_shape[-4:-1])\n",
    "    strides = tf.concat([tf.constant(strides), [1]], axis=0)\n",
    "    # Convolution layer output shape formula\n",
    "    output_shape = ((input_shape - kernel_shape) // strides) + 1\n",
    "    # Add filters \n",
    "    output_shape *= tf.concat([1, 1, kernel_shape[-1]], axis=0)\n",
    "    # Add vector dimension\n",
    "    output_shape = tf.concat([[-1, vector_dim], output_shape], axis=0)\n",
    "    return output_shape\n",
    "\n",
    "def get_output_shape(input_shape, kernel_shape, strides):\n",
    "    \"\"\"Get shape of output feature tensor\"\"\"\n",
    "    input_map_shape = tf.constant(input_shape[-3:-1])\n",
    "    input_depth_shape = tf.constant(input_shape[-1:])\n",
    "    kernel_map_shape = tf.constant(kernel_shape[-4:-2])\n",
    "    kernel_depth_shape = tf.constant(kernel_shape[-2:-1])\n",
    "    # Convolution layer result formula\n",
    "    output_map_shape = ((input_map_shape - kernel_map_shape) // strides) + 1\n",
    "    # Set output depth\n",
    "    output_depth_shape = ((input_depth_shape - kernel_depth_shape) + 1) * kernel_shape[-1]\n",
    "    output_shape = tf.concat([output_map_shape, output_depth_shape], axis=0)\n",
    "    return output_shape\n",
    "\n",
    "def iterate_sparsed_weight_indices(weight_shape, input_shape, output_shape, kernel_shape, strides, use_bias):\n",
    "    \"\"\"Iterate over indices of non-zero elements in sparsed weight matrix\"\"\"\n",
    "    for j in range(weight_shape[-2]):\n",
    "        # Compute position in output tensor\n",
    "        chan_num = j % output_shape[-1]\n",
    "        col_num = (j // output_shape[-1]) % output_shape[-2]\n",
    "        row_num = (j // output_shape[-1]) // output_shape[-2]\n",
    "        # Compute row in weight tensor to start with\n",
    "        offset = (row_num * input_shape[-2] * strides[-2] + col_num \\\n",
    "                * strides[-1]) * input_shape[-1] + chan_num\n",
    "        for i in range(tf.cast(weight_shape[-3], tf.int32) - offset):\n",
    "            # Compute position in input tensor\n",
    "            chan_num = i % input_shape[-1]\n",
    "            col_num = (i // input_shape[-1]) % input_shape[-2]\n",
    "            row_num = (i // input_shape[-1]) // input_shape[-2]\n",
    "            \n",
    "            if chan_num < kernel_shape[-2] and col_num < kernel_shape[-3] and row_num < kernel_shape[-4]:\n",
    "                for f in range(weight_shape[-1]):\n",
    "                    yield (i + int(offset), j, f)\n",
    "    \n",
    "    # Append bias\n",
    "    bias_i = int(weight_shape[-3] - 1)\n",
    "    for j in range(weight_shape[-2]):\n",
    "        for f in range(weight_shape[-1]):\n",
    "            yield (bias_i, j, f)\n",
    "\n",
    "\n",
    "def iterate_input_gather_indices(weight_shape, input_shape, output_shape, kernel_shape, strides, use_bias, vector_input=False):\n",
    "    position_nums = dict()\n",
    "    kernel_shape\n",
    "    \n",
    "    for j in range(weight_shape[-2]):\n",
    "        # Compute position in output tensor\n",
    "        chan_num = j % output_shape[-1]\n",
    "        col_num = (j // output_shape[-1]) % output_shape[-2]\n",
    "        row_num = (j // output_shape[-1]) // output_shape[-2]\n",
    "        # Compute row in weight tensor to start with\n",
    "        offset = (row_num * input_shape[-2] * strides[-2] + col_num \\\n",
    "                * strides[-1]) * input_shape[-1] + chan_num\n",
    "        \n",
    "        for i in range(tf.cast(weight_shape[-3], tf.int32) - offset):\n",
    "            # Compute position in input tensor\n",
    "            chan_num = i % input_shape[-1]\n",
    "            col_num = (i // input_shape[-1]) % input_shape[-2]\n",
    "            row_num = (i // input_shape[-1]) // input_shape[-2]\n",
    "            \n",
    "            if (chan_num < kernel_shape[-2] and col_num < kernel_shape[-3] and row_num < kernel_shape[-4]):\n",
    "                position = (i + int(offset),)\n",
    "                if vector_input:\n",
    "                    if position not in position_nums:\n",
    "                        position_nums[position] = 0\n",
    "                    yield position + (position_nums[position],)\n",
    "                    position_nums[position] += 1\n",
    "                else:\n",
    "                    yield position\n",
    "        # Set bias\n",
    "        if use_bias:\n",
    "            if vector_input:\n",
    "                yield (int(weight_shape[-3]-1), 0)\n",
    "            else:\n",
    "                yield (int(weight_shape[-3]-1),)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def get_sparsed_weight_params(input_shape, output_shape, kernel_shape, strides, use_bias, vector_input=False):\n",
    "    \"\"\"Get shape and indices of non-zero elements for sparsed weight matrix\"\"\"\n",
    "    output_shape = output_shape // tf.concat([[1, 1], kernel_shape[-1:]], axis=0)\n",
    "    # Compute weight shape\n",
    "    input_flat_len = tf.reduce_prod(input_shape) + (1 if use_bias else 0)\n",
    "    output_flat_len = tf.reduce_prod(output_shape)\n",
    "    weight_shape = tf.concat([input_flat_len, output_flat_len, tf.reduce_prod(kernel_shape[-1:])], axis=0)\n",
    "    weight_shape = tf.cast(weight_shape, tf.int64)\n",
    "    \n",
    "    # Get indices of non-zero elements in sparsed weight matrix\n",
    "    sparsed_indices = tf.constant(\n",
    "        list(iterate_sparsed_weight_indices(weight_shape, input_shape, output_shape, kernel_shape, strides, use_bias)),\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    gather_indices = tf.constant(\n",
    "        list(iterate_input_gather_indices(weight_shape, input_shape, output_shape, kernel_shape, strides, use_bias, vector_input)),\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    vector_dim = tf.reduce_prod(kernel_shape[:-1])\n",
    "    if use_bias:\n",
    "        # Bias must be taken into account\n",
    "        vector_dim += 1\n",
    "    gather_shape = tf.concat([tf.reduce_prod(output_shape), vector_dim, tf.constant(2 if vector_input else 1)], axis=0)\n",
    "    gather_indices = tf.reshape(gather_indices, gather_shape)\n",
    "    gather_indices = tf.transpose(gather_indices, perm=[1,0,2])\n",
    "    gather_shape = gather_indices.shape\n",
    "    \n",
    "    return weight_shape, sparsed_indices, gather_indices\n",
    "\n",
    "\n",
    "def get_dense_output_params(weight_shape, output_shape, kernel_shape, use_bias):\n",
    "    \"\"\"Get shape and indices of elements for dense output matrix\"\"\"\n",
    "    vector_dim = tf.reduce_prod(kernel_shape[:-1])\n",
    "    if use_bias:\n",
    "        # Bias must be taken into account\n",
    "        vector_dim += 1\n",
    "    shape = tf.concat([[vector_dim], output_shape], axis=0)\n",
    "    shape = tf.cast(shape, tf.int64)\n",
    "    \n",
    "    indices = [[v, i, j, f] for v in range(shape[-4]) for i in range(shape[-3]) for j in range(shape[-2]) for f in range(shape[-1])]\n",
    "    indices = tf.constant(\n",
    "        indices,\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    return shape, indices\n",
    "\n",
    "\n",
    "def concat_biases_fun(inputs_rank, axis=-1):\n",
    "    \"\"\"Add bias to each input vector\"\"\"\n",
    "    # Inputs shape can be partially known, so\n",
    "    # Get inputs slice with current dimension equals one\n",
    "    slice_begin = tf.zeros(inputs_rank, dtype=tf.int32)\n",
    "    slice_size = tf.concat([tf.fill([inputs_rank + axis], -1), tf.constant([1]), tf.fill([-axis - 1], -1)], 0)\n",
    "    \n",
    "    def concat_biases(inputs):\n",
    "        nonlocal slice_begin\n",
    "        nonlocal slice_size\n",
    "        nonlocal axis\n",
    "        inputs_slice = tf.slice(inputs, slice_begin, slice_size)\n",
    "        # Create biases shaped like inputs slice\n",
    "        biases = tf.ones_like(inputs_slice, dtype=inputs.dtype)\n",
    "        # Concatenate inputs with biases\n",
    "        return tf.concat([inputs, biases], axis)\n",
    "    \n",
    "    return concat_biases\n",
    "\n",
    "\n",
    "def multiply_sparsed(x_flat, weight, dense_indices, dense_shape):\n",
    "    \"\"\"Multiply separate (non-batched) input with sparsed weight tensor\"\"\"\n",
    "    output = x_flat * weight\n",
    "    # Rearrange and convet to dense\n",
    "    return tf.reshape(output.values, dense_shape)\n",
    "#     dense = sparse.SparseTensor(dense_indices, output.values, dense_shape)\n",
    "#     return sparse.to_dense(dense)\n",
    "\n",
    "\n",
    "def get_sparsed_weight(kernel, weight_shape, sparsed_indices, bias=None):\n",
    "    use_bias = bias is not None\n",
    "    \n",
    "    # Get kernel values\n",
    "    sparsed_values = tf.reshape(kernel, [-1])\n",
    "    sparsed_values = tf.tile(sparsed_values, weight_shape[-2:-1])\n",
    "    if use_bias:\n",
    "        # Get bias values\n",
    "        sparsed_bias = tf.reshape(bias, [-1])\n",
    "        sparsed_bias = tf.tile(sparsed_bias, weight_shape[-2:-1])\n",
    "        sparsed_values = tf.concat([sparsed_values, sparsed_bias], axis=0)\n",
    "    \n",
    "    # Initialize sparsed weight tensor\n",
    "    weight = sparse.SparseTensor(sparsed_indices, sparsed_values, weight_shape)\n",
    "    weight = sparse.reorder(weight)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "touched-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VInputConv(layers.Layer):\n",
    "    \"\"\"Input vector layer for convolutional networks\"\"\"\n",
    "    def __init__(self, filter_shape, num_filters=1, kernel_type=\"convolution\", strides=(1,1), padding=0, weight_initializer=\"random_normal\"):\n",
    "        super().__init__()\n",
    "        self.filter_shape = filter_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_type = kernel_type\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.weight_initializer = weight_initializer\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.kernel_type == \"convolution\":\n",
    "            kernel_shape = tf.concat([self.filter_shape, input_shape[-1:], [self.num_filters]], axis=0)\n",
    "            self.use_bias = True\n",
    "            bias_shape = kernel_shape[-1:]\n",
    "        else: # if kernel_type == \"pooling\"\n",
    "            kernel_shape = tf.concat([self.filter_shape, [1, 1]], axis=0)\n",
    "            self.use_bias = False\n",
    "        \n",
    "        self.kernel = self.add_weight(\n",
    "            shape=kernel_shape,\n",
    "            initializer=self.weight_initializer\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=bias_shape,\n",
    "                initializer=self.weight_initializer\n",
    "            )\n",
    "        \n",
    "        if self.padding > 0:\n",
    "            input_rank = input_shape.rank\n",
    "            self.paddings = tf.concat([tf.zeros([input_rank - 3], dtype=tf.int32), [self.padding, self.padding], [0]], axis=0)\n",
    "            self.paddings = tf.stack([self.paddings, self.paddings], axis=1)\n",
    "        \n",
    "        self.full_output_shape = get_full_output_shape(input_shape, kernel_shape, self.strides, self.use_bias)\n",
    "        \n",
    "        padded_input_shape = get_input_shape(input_shape, self.padding)\n",
    "        output_shape = get_output_shape(padded_input_shape, kernel_shape, self.strides)\n",
    "        \n",
    "        self.weight_shape, self.sparsed_indices, self.gather_indices = get_sparsed_weight_params(\n",
    "            padded_input_shape, output_shape, kernel_shape, self.strides, self.use_bias\n",
    "        )\n",
    "        self.dense_shape, self.dense_indices = get_dense_output_params(\n",
    "            self.weight_shape, output_shape, kernel_shape, self.use_bias\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.flattened_weight = tf.reshape(self.kernel, tf.concat([tf.reduce_prod(self.kernel.shape[:-1]), 1, self.kernel.shape[-1]], axis=0))\n",
    "        if self.use_bias:\n",
    "            # Get bias values\n",
    "            bias = tf.reshape(self.bias, tf.concat([1, 1, self.bias.shape[-1]], axis=0))\n",
    "            self.flattened_weight = tf.concat([self.flattened_weight, bias], axis=0)\n",
    "            \n",
    "        self.weight_shape = tf.concat([[1], self.gather_indices.shape[-2:-1], [1]], axis=0)\n",
    "        \n",
    "        self.concat_biases = concat_biases_fun(len(input_shape) - 2, axis=-1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        if self.padding > 0:\n",
    "            x = tf.pad(x, self.paddings)\n",
    "        \n",
    "        \n",
    "#         x_flat = tf.expand_dims(tf.expand_dims(flatten(x, tf.constant(3)), -1), -1)\n",
    "#         if self.use_bias:\n",
    "#             x_flat = concat_biases(x_flat, axis=-3)\n",
    "        \n",
    "        x_flat = flatten(x, tf.constant(3))\n",
    "        if self.use_bias:\n",
    "            x_flat = self.concat_biases(x_flat)\n",
    "        \n",
    "#         x = tf.map_fn(\n",
    "#             lambda x: tf.gather_nd(x, self.gather_indices), \n",
    "#             x_flat\n",
    "#         )\n",
    "        x = tf.transpose(x_flat, perm=[1,0])\n",
    "        x = tf.gather_nd(x, self.gather_indices) # TODO: depends on rank\n",
    "        x = tf.transpose(x, perm=[2,0,1])\n",
    "        \n",
    "        x = tf.expand_dims(x, -1)\n",
    "    \n",
    "        weight = tf.tile(self.flattened_weight, self.weight_shape)\n",
    "        \n",
    "        y = x * weight\n",
    "        print(y.shape)\n",
    "        print(self.full_output_shape)\n",
    "        return tf.reshape(y, self.full_output_shape)\n",
    "        #gather_indices\n",
    "        \n",
    "#         weight = get_sparsed_weight(\n",
    "#             self.kernel, \n",
    "#             self.weight_shape, \n",
    "#             self.sparsed_indices, \n",
    "#             bias=self.bias if self.use_bias else None\n",
    "#         )\n",
    "        \n",
    "#         output = tf.map_fn(\n",
    "#             lambda x: multiply_sparsed(x, weight, self.dense_indices, self.dense_shape), \n",
    "#             x_flat\n",
    "#         )\n",
    "        \n",
    "#         return output\n",
    "\n",
    "    \n",
    "class VOutputConv(layers.Layer):\n",
    "    \"\"\"Output vector layer for convolutional networks\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "smooth-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 9, 64, 2)\n",
      "tf.Tensor([-1  9  8  8  2], shape=(5,), dtype=int32)\n",
      "(64, 9, 8, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "# import timeit\n",
    "batch_size = 64\n",
    "x_dim = 9\n",
    "x = tf.reshape(tf.range([batch_size*x_dim*x_dim*2], dtype=tf.float32), shape=(batch_size,x_dim,x_dim,2))\n",
    "strides = (1,1)\n",
    "padding = 0\n",
    "num_filters=2\n",
    "filter_dim=2\n",
    "\n",
    "tries = 100\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = VInputConv((filter_dim,filter_dim), num_filters=num_filters, kernel_type=\"convolution\", strides=strides, padding=padding)\n",
    "print(layer(x).shape)\n",
    "# new_time = timeit.timeit(lambda: layer(x), number=tries)\n",
    "\n",
    "# layer = layers.Conv2D(num_filters, filter_dim, activation='relu', strides=strides, padding=\"valid\")\n",
    "# print(layer(x).shape)\n",
    "# old_time = timeit.timeit(lambda: layer(x), number=tries)\n",
    "\n",
    "# print(new_time / old_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "honest-prince",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8, 8, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03154229999927338"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.Conv2D(num_filters, filter_dim, activation='relu', strides=strides, padding=\"valid\")\n",
    "print(layer(x).shape)\n",
    "timeit.timeit(lambda: layer(x), number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "shared-investment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0.8125 0.8125]\n",
      "   [0.875  0.875 ]]\n",
      "\n",
      "  [[0.9375 0.9375]\n",
      "   [1.     1.    ]]]\n",
      "\n",
      "\n",
      " [[[1.1875 1.1875]\n",
      "   [1.25   1.25  ]]\n",
      "\n",
      "  [[1.3125 1.3125]\n",
      "   [1.375  1.375 ]]]], shape=(2, 2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.reshape(tf.range([2*3*3*2], dtype=tf.float32), shape=(2,3,3,2))\n",
    "# kernel = tf.constant([[[[1]], [[2]]], [[[3]], [[4]]]], dtype=tf.float32)\n",
    "kernel = tf.Variable(tf.reshape(tf.range([2*2*2*2], dtype=tf.float32), shape=(2,2,2,2)))\n",
    "strides = (1,1)\n",
    "padding = 0\n",
    "\n",
    "input_shape = get_input_shape(x.shape, padding)\n",
    "output_shape = get_output_shape(input_shape, kernel.shape, strides, padding)\n",
    "\n",
    "weight_shape, sparsed_indices = get_sparsed_weight_params(input_shape, output_shape, kernel.shape, strides)\n",
    "\n",
    "dense_shape, dense_indices = get_dense_output_params(weight_shape, output_shape, kernel.shape, False)\n",
    "\n",
    "def multiply_sparsed(x_flat):\n",
    "    \"\"\"Bad code, but sparse-to-dence broadcasting is not working\"\"\"\n",
    "    output = x_flat * weight\n",
    "    dense = sparse.SparseTensor(dense_indices, output.values, dense_shape)\n",
    "    return sparse.to_dense(dense)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    sparsed_values = tf.reshape(kernel, [-1])\n",
    "    sparsed_values = tf.tile(sparsed_values, weight_shape[-2:-1])\n",
    "    weight = sparse.SparseTensor(sparsed_indices, sparsed_values, weight_shape)\n",
    "    weight = sparse.reorder(weight)\n",
    "    x_flat = tf.expand_dims(tf.expand_dims(flatten(x, tf.constant(3)), -1), -1)\n",
    "    result = tf.map_fn(multiply_sparsed, x_flat)\n",
    "    loss = tf.reduce_mean(result)\n",
    "\n",
    "print(tape.gradient(loss, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infectious-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = tf.keras.layers.Conv2D(2,3,activation=\"relu\", input_shape=(28,28,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "german-china",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.14504266, -0.27120164],\n",
       "          [-0.28834227,  0.0261265 ],\n",
       "          [ 0.07041037, -0.2354877 ]],\n",
       " \n",
       "         [[-0.19154891,  0.3179202 ],\n",
       "          [ 0.01622459,  0.1081526 ],\n",
       "          [-0.3642441 , -0.32064518]],\n",
       " \n",
       "         [[-0.20420814,  0.02284172],\n",
       "          [-0.20973232, -0.03993881],\n",
       "          [-0.32509318,  0.01819271]]],\n",
       " \n",
       " \n",
       "        [[[ 0.16470277,  0.2110464 ],\n",
       "          [ 0.22549063, -0.0138002 ],\n",
       "          [-0.10054016, -0.2771492 ]],\n",
       " \n",
       "         [[-0.12416469, -0.32693505],\n",
       "          [ 0.23607647, -0.23215224],\n",
       "          [ 0.02976117, -0.17566273]],\n",
       " \n",
       "         [[-0.0430896 ,  0.20499647],\n",
       "          [-0.10320622, -0.16313514],\n",
       "          [-0.2966457 , -0.3647445 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.31194293,  0.28944844],\n",
       "          [ 0.2014187 , -0.3381625 ],\n",
       "          [ 0.10150501, -0.28951746]],\n",
       " \n",
       "         [[-0.10838193, -0.25806308],\n",
       "          [ 0.26325548, -0.02701598],\n",
       "          [ 0.00530359,  0.21974504]],\n",
       " \n",
       "         [[ 0.14539486,  0.32093298],\n",
       "          [-0.05022594, -0.30173424],\n",
       "          [-0.09479502,  0.0414812 ]]]], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(tf.reshape(tf.range(28*28*3, dtype=tf.float32) * 0.1, (1, 28,28,3)))\n",
    "conv.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
